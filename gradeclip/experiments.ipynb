{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f752039-6292-4863-b5a2-7cf675327edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import shapiq\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Game_MM_CLIP.clip as mm_clip\n",
    "\n",
    "from generate_emap import clipmodel, preprocess, mm_clipmodel, mm_interpret, gradeclip, gradeclip_text, clip_encode_dense, clip_encode_text_dense\n",
    "\n",
    "sys.path.append('../')\n",
    "import src\n",
    "PATH = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb7e86-910c-4027-b1ca-f02f835308d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_image(mode, input_image, input_text):\n",
    "    image_preprocessed = preprocess(input_image).to(device).unsqueeze(0)\n",
    "    if \"gradeclip\" in mode:\n",
    "        text_processed = clip.tokenize([input_text]).to(device)\n",
    "        text_embedding = clipmodel.encode_text(text_processed)\n",
    "        text_embedding = F.normalize(text_embedding, dim=-1)\n",
    "        outputs, v_final, last_input, v, q_out, k_out, attn, att_output, map_size = clip_encode_dense(image_preprocessed)\n",
    "        image_embedding = F.normalize(outputs[:, 0], dim=-1)\n",
    "        cosines = (image_embedding @ text_embedding.T)[0]\n",
    "        emap = [gradeclip(c, q_out, k_out, v, att_output, map_size, withksim=True) for c in cosines]\n",
    "        emap = torch.stack(emap, dim=0).sum(0) \n",
    "    elif \"game\" in mode:\n",
    "        text_tokenized = mm_clip.tokenize([input_text]).to(device)\n",
    "        emap = mm_interpret(model=mm_clipmodel, image=image_preprocessed, texts=text_tokenized, device=device)    \n",
    "        emap = emap.sum(0) \n",
    "    return emap\n",
    "\n",
    "def explain_text(mode, input_image, input_text):\n",
    "    image_preprocessed = preprocess(input_image).to(device).unsqueeze(0)\n",
    "    if \"gradeclip\" in mode:\n",
    "        image_embedding = clipmodel.encode_image(image_preprocessed)\n",
    "        image_embedding = F.normalize(image_embedding, dim=-1)\n",
    "        text_processed = clip.tokenize([input_text]).to(device)\n",
    "        x, (qs, ks, vs), attns, atten_outs = clip_encode_text_dense(text_processed, n=8)\n",
    "        text_embedding = F.normalize(x, dim=-1)\n",
    "        cosine = (image_embedding @ text_embedding.T)\n",
    "        eos_position = text_processed.argmax(dim=-1)\n",
    "        emap = gradeclip_text(cosine[0], qs, ks, vs, atten_outs, eos_position, withksim=True)\n",
    "    elif \"game\" in mode:\n",
    "        text_tokenized = mm_clip.tokenize([input_text]).to(device)\n",
    "        emap = mm_interpret(model=mm_clipmodel, image=image_preprocessed, texts=text_tokenized, device=device, flag=\"text\")    \n",
    "        id_cls = text_tokenized.argmax(dim=-1).item()\n",
    "        r_text = emap[0][id_cls, 1:id_cls]\n",
    "        emap = r_text.flatten()\n",
    "    return emap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372df7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img, mean, std):\n",
    "    return img * torch.tensor(std).view(3, 1, 1) + torch.tensor(mean).view(3, 1, 1)\n",
    "image_mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "image_std = (0.26862954, 0.26130258, 0.27577711)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cb35b",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"black dog next to a yellow hydrant\"\n",
    "input_image = Image.open(os.path.join(\"..\", \"assets\", \"dog_and_hydrant.png\"))\n",
    "values_image = explain_image(\"game\", input_image, input_text).reshape(-1)\n",
    "values_text = explain_text(\"game\", input_image, input_text)\n",
    "values = np.concat([values_image.detach().cpu().numpy(), values_text.detach().cpu()])\n",
    "gv = src.utils.convert_array_to_first_order(values)\n",
    "gv.save(\"../results/dog_and_hydrant_game.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"black dog next to a yellow hydrant\"\n",
    "input_image = Image.open(os.path.join(\"..\", \"assets\", \"dog_and_hydrant.png\"))\n",
    "values_image = explain_image(\"gradeclip\", input_image, input_text).reshape(-1)\n",
    "values_text = explain_text(\"gradeclip\", input_image, input_text)\n",
    "values = np.concat([values_image.detach().cpu().numpy(), values_text.detach().cpu()])\n",
    "gv = src.utils.convert_array_to_first_order(values)\n",
    "gv.save(\"../results/dog_and_hydrant_gradeclip.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9d815",
   "metadata": {},
   "source": [
    "### insertion/deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15064a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RESULTS = \"../results\"\n",
    "PATH_INPUT = \"../results/mscoco\"\n",
    "# MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfad5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"clip-benchmark/wds_mscoco_captions\",\n",
    "    split=\"test\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, stop = 0, 1000\n",
    "df_results = pd.read_csv(os.path.join(PATH_RESULTS, MODEL_NAME, \"mscoco_predictions.csv\"), index_col=0)\n",
    "top_ids = df_results.sort_values(\"logit\", ascending=False).iloc[start:stop, :].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba90c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_details = {}\n",
    "for mode in ['game', 'gradeclip']:\n",
    "    results_details[f'{mode}'] = {}\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'input': [], \n",
    "    'mode': [], \n",
    "    'mean': [],\n",
    "    'mean_normalized': []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "for i, d in enumerate(dataset):\n",
    "    if i not in top_ids:\n",
    "        continue\n",
    "    n_iter += 1\n",
    "    if n_iter % 5 == 1:\n",
    "        print(f'iter: {start + n_iter}/{stop}', flush=True)\n",
    "\n",
    "    input_image = d['jpg']\n",
    "    input_text = d['txt'].split(\"\\n\")[df_results.loc[i, \"best_text_id\"].item()]\n",
    "    \n",
    "    game = src.game_openai.CLIPGame(\n",
    "        clipmodel, preprocess, \n",
    "        input_image=input_image,\n",
    "        input_text=input_text,\n",
    "        patch_size=16 if MODEL_NAME.endswith(\"16\") else 32,\n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    for mode in ['game', 'gradeclip']:\n",
    "\n",
    "        path_file = os.path.join(PATH_INPUT, MODEL_NAME, mode, f'iv_order1_{i}.pkl')\n",
    "        iv_object = shapiq.InteractionValues.load(path_file)\n",
    "        \n",
    "        attribution_values = iv_object.get_n_order(1).values\n",
    "        attribution_values_sorted = np.sort(attribution_values)\n",
    "        # insertion / deletion, most important first / least important first\n",
    "        coalition_matrix_deletion_mif = np.stack([attribution_values <= v for v in attribution_values_sorted[::-1]] + [game.empty_coalition])\n",
    "        predictions_deletion_mif = game.value_function(coalition_matrix_deletion_mif)\n",
    "        coalition_matrix_deletion_lif = np.stack([attribution_values >= v for v in attribution_values_sorted] + [game.empty_coalition])\n",
    "        predictions_deletion_lif = game.value_function(coalition_matrix_deletion_lif)\n",
    "\n",
    "        results_details[f'{mode}'][i] = {\n",
    "            'predictions_deletion_mif': predictions_deletion_mif,\n",
    "            'predictions_deletion_lif': predictions_deletion_lif,\n",
    "        }\n",
    "\n",
    "        assert predictions_deletion_mif[-1] == predictions_deletion_lif[-1]\n",
    "        assert predictions_deletion_mif[0] == predictions_deletion_lif[0]\n",
    "        \n",
    "        # normalize the curve\n",
    "        min_value = predictions_deletion_mif[-1]\n",
    "        max_value = predictions_deletion_mif[0]\n",
    "\n",
    "        predictions_deletion_mif_01 = (predictions_deletion_mif - min_value) / (max_value - min_value)\n",
    "        predictions_deletion_lif_01 = (predictions_deletion_lif - min_value) / (max_value - min_value)\n",
    "\n",
    "        results = pd.concat([results, pd.DataFrame({\n",
    "            'input': [i], \n",
    "            'mode': [mode], \n",
    "            'mean': [np.mean(predictions_deletion_lif - predictions_deletion_mif)],\n",
    "            'mean_normalized': [np.mean(predictions_deletion_lif_01 - predictions_deletion_mif_01)]\n",
    "        })])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby([\"mode\"]).aggregate({\n",
    "    'mean': ['mean', 'std'],\n",
    "    'mean_normalized': ['mean', 'std'],\n",
    "}).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81417cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(PATH_RESULTS, MODEL_NAME, \"mscoco_aid_game_gradeclip.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(PATH_RESULTS, MODEL_NAME, \"mscoco_aid_game_gradeclip.npy\"), results_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae888b",
   "metadata": {},
   "source": [
    "### visualize explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b35772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "PATH_INPUT = \"../results\"\n",
    "PATH_OUTPUT = \"../results/mscoco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"clip-benchmark/wds_mscoco_captions\",\n",
    "    split=\"test\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "_tokenizer = clip.simple_tokenizer.SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bde2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(os.path.join(PATH_INPUT, MODEL_NAME, \"mscoco_predictions.csv\"), index_col=0)\n",
    "top_ids = df_results.sort_values(\"logit\").tail(1000).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\n",
    "    'gradeclip', \n",
    "    'game'\n",
    "]:\n",
    "    path_output = os.path.join(PATH_OUTPUT, MODEL_NAME, mode)\n",
    "    if not os.path.exists(path_output):\n",
    "        os.makedirs(path_output)\n",
    "\n",
    "    print_counter = 0\n",
    "    for i, d in enumerate(dataset):\n",
    "        if i not in top_ids:\n",
    "            continue\n",
    "        if print_counter % 100 == 0:\n",
    "            print(f'{mode} | iter: {print_counter}/1000', flush=True)\n",
    "        print_counter += 1\n",
    "\n",
    "        input_image = d['jpg']\n",
    "        input_text = d['txt'].split(\"\\n\")[df_results.loc[i, \"best_text_id\"].item()]\n",
    "\n",
    "        values_image = explain_image(mode, input_image, input_text).reshape(-1)\n",
    "        values_text = explain_text(mode, input_image, input_text)\n",
    "        values = np.concat([values_image.detach().cpu().numpy(), values_text.detach().cpu()])\n",
    "        gv = src.utils.convert_array_to_first_order(values, index=mode)\n",
    "        gv.save(os.path.join(path_output, f'iv_order1_{i}.pkl'))\n",
    "        ##\n",
    "        text_processed = clip.tokenize([input_text])\n",
    "        text_tokens = _tokenizer.encode(input_text)\n",
    "        text_tokens_decoded = [_tokenizer.decode([a]) for a in text_tokens]\n",
    "        image_preprocessed = preprocess(input_image)\n",
    "        input_image_denormalized = denormalize(image_preprocessed, image_mean, image_std)\n",
    "        input_image_denormalized = input_image_denormalized.permute(1, 2, 0).numpy()\n",
    "        fig = src.plots.plot_image_and_text_together(\n",
    "            img=input_image_denormalized, \n",
    "            text=text_tokens_decoded, \n",
    "            image_players=list(range(len(values_image))), \n",
    "            iv=gv, \n",
    "            normalize_jointly=False,\n",
    "            figsize=(8, 8),\n",
    "            show=False\n",
    "        ) \n",
    "        fig.suptitle(f'{MODEL_NAME} {mode}', fontsize=20, y=1.05)\n",
    "        fig.savefig(os.path.join(path_output, f'ex_order1_{i}.png'), bbox_inches='tight')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6673f7",
   "metadata": {},
   "source": [
    "### pointing game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "GAMES = [\n",
    "    ['goldfish', 'husky', 'pizza', 'tractor'],\n",
    "    ['cat', 'goldfish', 'plane', 'pizza'],\n",
    "    ['banana', 'cat', 'tractor', 'ball'],\n",
    "    ['husky', 'banana', 'plane', 'church'],\n",
    "    ['pizza', 'ipod', 'goldfish', 'banana'],\n",
    "    ['ipod', 'cat', 'husky', 'plane'],\n",
    "    ['tractor', 'ball', 'banana', 'ipod'],\n",
    "    ['plane', 'church', 'ball', 'goldfish'],\n",
    "    ['church', 'pizza', 'ipod', 'cat'],\n",
    "    ['ball', 'husky', 'banana', 'tractor'],\n",
    "]\n",
    "PATH_OUTPUT = \"../results/imagenet_pointing_game\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in ['gradeclip', 'game']:\n",
    "    for game in GAMES:\n",
    "        PATH_INPUT = f'../data/imagenet_pointing_game/{\"_\".join(game)}'\n",
    "        for i_objects in range(1, 5):\n",
    "            class_labels = game[:i_objects]\n",
    "            cl = \"_\".join(class_labels)\n",
    "            input_text = cl.replace(\"_\", \" \")\n",
    "            path_output = os.path.join(PATH_OUTPUT, MODEL_NAME, mode, cl)\n",
    "            if not os.path.exists(path_output):\n",
    "                os.makedirs(path_output)\n",
    "            for i_image in range(50):\n",
    "                input_image = Image.open(os.path.join(PATH_INPUT, f'{i_image}.jpg'))\n",
    "                values_image = explain_image(mode, input_image, input_text).reshape(-1)\n",
    "                values_text = explain_text(mode, input_image, input_text)\n",
    "                values = np.concat([values_image.detach().cpu().numpy(), values_text.detach().cpu()])\n",
    "                gv = src.utils.convert_array_to_first_order(values)\n",
    "                gv.save(os.path.join(path_output, f'iv_order1_{i_image}.pkl'))\n",
    "                ##\n",
    "                image_preprocessed = preprocess(input_image)\n",
    "                input_image_denormalized = denormalize(image_preprocessed, image_mean, image_std)\n",
    "                input_image_denormalized = input_image_denormalized.permute(1, 2, 0).numpy()\n",
    "                fig = src.plots.plot_image_and_text_together(\n",
    "                    img=input_image_denormalized, \n",
    "                    text=cl.split(\"_\"), \n",
    "                    image_players=list(range(len(values_image))), \n",
    "                    iv=gv, \n",
    "                    normalize_jointly=True,\n",
    "                    figsize=(6, 6),\n",
    "                    show=False\n",
    "                ) \n",
    "                fig.suptitle(f'{MODEL_NAME} {mode}', fontsize=20, y=1.05)\n",
    "                fig.savefig(os.path.join(path_output, f'ex_order1_{i_image}.png'), bbox_inches='tight')\n",
    "                plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmti-gradeclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
